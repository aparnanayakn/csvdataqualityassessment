{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import ntpath\n",
    "import re\n",
    "import time\n",
    "\n",
    "from stat import *\n",
    "from rdflib import *\n",
    "from rdflib.namespace import *\n",
    "from langdetect import detect\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from langdetect import DetectorFactory, detect  #to enforce consistent results\n",
    "\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import shapiro \n",
    "\n",
    "import random\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langdetect.lang_detect_exception import LangDetectException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "def generate_random_dataframe(n, m):\n",
    "    data = {}\n",
    "    for i in range(m):\n",
    "        col_type = random.choice([int, float, str])\n",
    "        if col_type == int:\n",
    "            mean, std_dev = random.randint(0, 50), random.randint(1, 10)\n",
    "            data[f'col_{i}'] = np.random.normal(mean, std_dev, n).astype(int)\n",
    "            outliers = np.random.normal(mean * 3, std_dev * 3, int(n * 0.05)).astype(int)\n",
    "            data[f'col_{i}'][:len(outliers)] = outliers\n",
    "            print(len(outliers))\n",
    "        elif col_type == float:\n",
    "            mean, std_dev = random.uniform(0, 50), random.uniform(1, 10)\n",
    "            data[f'col_{i}'] = np.random.normal(mean, std_dev, n)\n",
    "            outliers = np.random.normal(mean * 3, std_dev * 3, int(n * 0.05))\n",
    "            data[f'col_{i}'][:len(outliers)] = outliers\n",
    "            print(len(outliers))\n",
    "        elif col_type == str:\n",
    "            data[f'col_{i}'] = [''.join(random.choices(string.ascii_letters, k=5)) for _ in range(n)]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Uncheck the following to generate random dataset\n",
    "df = generate_random_dataframe(250,5)\n",
    "#df.to_csv(\"randomData5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNormality(columnValues):\n",
    "    p_value = None\n",
    "    if(len(columnValues) <= 50):\n",
    "        _, p_value = shapiro(columnValues)\n",
    "    else:\n",
    "        _, p_value = (kstest(columnValues, 'norm'))\n",
    "    if(p_value < 0.05):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[^a-zA-Z0-9]') #removing non-alpha numeric characters from file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datatype RE\n",
    "intType = re.compile(r\"^\\d+$\")\n",
    "dateType1 = re.compile(r\"^\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}$\")\n",
    "dateType2 = re.compile(r\"^\\d{1,2}[-/]\\d{1,2}[-/]\\d{4}$\")\n",
    "stringType = re.compile(\"^[a-zA-Z]+.*\\s*[a-zA-Z]*$\")\n",
    "floatType = re.compile(r\"^[+-]?(\\d*\\.\\d+|\\d+\\.?\\d*)([eE][+-]?\\d+)?$\")\n",
    "uriType = re.compile(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predcting column datatype based on highest occurance of value\n",
    "def typeCheck(singleCol):\n",
    "    ci=cs=co=cf=cd=cu=0\n",
    "    singleCol.fillna(\"$#\", inplace = True)    #replace all NA with special characters\n",
    "    for i in range(len(singleCol)):\n",
    "        if((uriType.match(str(singleCol[i])))):\n",
    "            cu+=1\n",
    "        elif(stringType.match(str(singleCol[i]))):\n",
    "            cs+=1\n",
    "        elif((intType.match(str(singleCol[i]) ))):\n",
    "            ci+=1\n",
    "        elif(dateType1.match(str(singleCol[i]) or dateType2.match(str(singleCol[i])))):\n",
    "            cd+=1\n",
    "        elif(floatType.match(str(singleCol[i])) and singleCol[i]!='$#' ):\n",
    "            cf+=1\n",
    "        else:\n",
    "            co+=1\n",
    "    daConsidered=['int','str','float','date','uri','other']\n",
    "    overall=[ci,cs,cf,cd,cu,co]\n",
    "    di=zip(daConsidered, overall)\n",
    "    #actDatatype=max(di)[0]\n",
    "    if cf > ci :             #column with float values, int gets assigned to ci, coverting it to cf\n",
    "        cf = cf+ci\n",
    "        ci=0\n",
    "    return overall.index(max(overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting language in case the column is string\n",
    "def detectLang(singleCol):\n",
    "    DetectorFactory.seed = 0\n",
    "    lang = []   \n",
    "    for r in singleCol:\n",
    "        lang.append(detect(r))\n",
    "    c=Counter(lang)\n",
    "    totalItems=len(lang)\n",
    "    return(c.most_common(1)[0][0],(c.most_common(1)[0][1]/totalItems)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No typos in date\n",
    "def is_valid_date(year, month, day):\n",
    "    day_count_for_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    if year%4==0 and (year%100 != 0 or year%400==0):\n",
    "        day_count_for_month[2] = 29\n",
    "    return (1 <= month <= 12 and 1 <= day <= int(day_count_for_month[month]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count all NA's\n",
    "def countNA(singleCol):\n",
    "    resultedCounter = Counter(singleCol)\n",
    "    return (resultedCounter[' ']+singleCol.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting valid date \n",
    "def checkDate(singleCol):\n",
    "    year = pd.DatetimeIndex(singleCol).year  \n",
    "    month = pd.DatetimeIndex(singleCol).month\n",
    "    day = pd.DatetimeIndex(singleCol).day\n",
    "    validDate = []\n",
    "    for i in range(len(year)):\n",
    "        validDate.append(is_valid_date(year[i], month[i], day[i]))\n",
    "    trueCount = sum(validDate)\n",
    "    if(trueCount == len(year)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_header(path, n=5, threshold=0.9):\n",
    "    # Read n rows with and without a header\n",
    "    df1 = pd.read_csv(path, header='infer', nrows=n)  # Assuming first row is the header\n",
    "    df2 = pd.read_csv(path, header=None, nrows=n)     # Treating all rows as data\n",
    "    \n",
    "    # Get the first row from df2 (as data, not header) and align indexes with df1.dtypes\n",
    "    first_row = df2.iloc[0]\n",
    "    first_row.index = df1.dtypes.index  # Align indexes\n",
    "    \n",
    "    # Compare the data types of the first row with the inferred data types\n",
    "    sim = (first_row.apply(type) == df1.dtypes).mean()  # Calculate similarity\n",
    "    return sim < threshold\n",
    "\n",
    "# Extract column names\n",
    "def getSchema(csv_file):\n",
    "    columnNames = []\n",
    "    if identify_header(csv_file):  \n",
    "        columnNames = list(pd.read_csv(csv_file, nrows=0).columns)  # Get header names\n",
    "        has_header = 1\n",
    "    else:\n",
    "        # Generate column names like Attr1, Attr2, ...\n",
    "        total_columns = pd.read_csv(csv_file, header=None, nrows=1).shape[1]\n",
    "        columnNames = [f\"Attr{i+1}\" for i in range(total_columns)]\n",
    "        has_header = 0\n",
    "    return has_header, columnNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating mean and std\n",
    "def truncate(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return int(n * multiplier) / multiplier\n",
    "def meanStd(columnValue):\n",
    "    return truncate(np.mean(columnValue),2),truncate(np.std(columnValue),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/d19125691/Experiments/Experiments/csv2rdf/csvtordfgit/csvdataqualityassessment/preprocessing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVW = Namespace(\"http://www.w3.org/ns/csvw#\")\n",
    "CSVTORDF = Namespace(\"https://purl.archive.org/domain/csvtordf#\")\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "#ontology file location\n",
    "g.parse(\"./csvtordf.owl\")\n",
    "g.bind(\"csvw\", CSVW)\n",
    "g.bind(\"csvtordf\", CSVTORDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_file = \"/home/d19125691/Experiments/Experiments/csv2rdf/csv2rdfworking/datasets/education.csv\"\n",
    "csv_file = \"/home/d19125691/Experiments/Experiments/csv2rdf/csvtordfgit/csvdataqualityassessment/datasets/iris.csv\"\n",
    "\n",
    "nocolumnheader = 0\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "nocolumnheader, columnNames = getSchema(csv_file)\n",
    "\n",
    "cleanedNames = []\n",
    "for cN in columnNames:\n",
    "    cleanedNames.append(pattern.sub('', cN))\n",
    "\n",
    "columnNames = cleanedNames\n",
    "\n",
    "with open(csv_file, mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    if(nocolumnheader):\n",
    "        header = next(reader)\n",
    "    rows = [row for row in reader] \n",
    "    for i, row in enumerate(reader):\n",
    "        rows.append(row)\n",
    "file_name = os.path.basename(csv_file)\n",
    "\n",
    "fileName = os.path.splitext(file_name)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#efine the file path\n",
    "#csv_file = \"/home/d19125691/Experiments/Experiments/csv2rdf/csv2rdfworking/preprocessing/iris_sample.csv\"\n",
    "\n",
    "# Initialize variables\n",
    "#nocolumnheader = 0\n",
    "#\n",
    "#df = pd.read_csv(csv_file)\n",
    "\n",
    "# getSchema provides the schema of the file\n",
    "#nocolumnheader, columnNames = getSchema(csv_file)\n",
    "\n",
    "# Clean column name\n",
    "#cleanedNames = [re.sub(r'\\W+', '', cN) for cN in columnNames]\n",
    "#columnNames = cleanedNames\n",
    "\n",
    "#rows = []\n",
    "#with open(csv_file, mode='r') as file:\n",
    " #   reader = csv.reader(file)\n",
    "  #  if nocolumnheader:\n",
    "   #     print(\"NO\")\n",
    "    #    header = next(reader)\n",
    "    #for i, row in enumerate(reader):\n",
    "     #   rows.append(row)\n",
    "\n",
    "#file_name = os.path.basename(csv_file)\n",
    "#fileName = file_name[0]  # Extract the first character of the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.w3.org/ns/csvw#Column_0 http://www.w3.org/2001/XMLSchema#float\n",
      "http://www.w3.org/ns/csvw#Column_1 http://www.w3.org/2001/XMLSchema#float\n",
      "http://www.w3.org/ns/csvw#Column_2 http://www.w3.org/2001/XMLSchema#float\n",
      "http://www.w3.org/ns/csvw#Column_3 http://www.w3.org/2001/XMLSchema#float\n",
      "http://www.w3.org/ns/csvw#Column_4 http://www.w3.org/2001/XMLSchema#string\n"
     ]
    }
   ],
   "source": [
    "count=0   #count triples\n",
    "flag=0\n",
    "countlongURI = 0 #counting Long URI if any \n",
    "count=0   #count triples\n",
    "completeness = 0  #population completeness in the file\n",
    "completeFlag = 0\n",
    "\n",
    "completeDataset = 0\n",
    "totalmissingvalues = df.isna().sum().sum() / (df.shape[0]*df.shape[1])\n",
    "\n",
    "table_uri = CSVW.Table+fileName\n",
    "completeness_uri = CSVW.Completeness\n",
    "\n",
    "g.add((table_uri, RDF.type, CSVW.Table))\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    flag=0\n",
    "    column_uri = CSVW.Column+\"_\"+columnNames[i]\n",
    "    completeness = countNA(df.iloc[:,i])\n",
    "    if(completeness):\n",
    "        constraint_node = BNode()\n",
    "        g.add((constraint_node, RDF.type, CSVTORDF.DataQuality))\n",
    "        g.add((column_uri, CSVTORDF.hasConstraint, constraint_node))\n",
    "        g.add((constraint_node, CSVTORDF.constraintType, completeness_uri))\n",
    "        g.add((constraint_node, CSVTORDF.constraintElement, Literal(completeness, datatype=XSD.float)))\n",
    "\n",
    "    colDT = typeCheck(df.iloc[:, i])\n",
    "    if(colDT==0):\n",
    "        datatype = XSD.integer\n",
    "        mean,std = meanStd(df.iloc[:, i])\n",
    "        flag=1\n",
    "    elif(colDT==1):\n",
    "        flag=2\n",
    "        datatype = XSD.string\n",
    "    elif(colDT==2):\n",
    "        datatype = XSD.float\n",
    "        mean,std = meanStd(df.iloc[:, i])\n",
    "        flag = 1\n",
    "    elif(colDT==3):\n",
    "        datatype = XSD.dateTime\n",
    "        flag=3\n",
    "    elif(colDT==4):\n",
    "        datatype=XSD.anyURI\n",
    "        flag=4\n",
    "    else:\n",
    "        flag = 4\n",
    "        datatype = XSD.string\n",
    "    \n",
    "    g.add((column_uri, RDF.type, CSVW.Column))\n",
    "    g.add((column_uri, CSVW.datatype, datatype))\n",
    "    print(column_uri, datatype)\n",
    "    count=count+2   \n",
    "    \n",
    "    if(flag==1):\n",
    "        result = testNormality(df.iloc[:,i])\n",
    "        if(result):\n",
    "            g.add((column_uri, CSVTORDF.mean, Literal(mean, datatype=XSD.float) ))\n",
    "            g.add((column_uri, CSVTORDF.stdDev, Literal(std, datatype=XSD.float) ))\n",
    "        else:\n",
    "            Q1 = np.percentile(df.iloc[:,i], 25)\n",
    "            Q3 = np.percentile(df.iloc[:,i], 75)\n",
    "            g.add((column_uri, CSVTORDF.Q1, Literal(Q1, datatype=XSD.float) ))\n",
    "            g.add((column_uri, CSVTORDF.Q3, Literal(Q3, datatype=XSD.float) ))\n",
    "        count+=2\n",
    "    if(flag==2 and completeFlag != 1):\n",
    "        try:\n",
    "            lang, percentage = detectLang(df.iloc[:,i])\n",
    "            g.add((column_uri, CSVTORDF.lang, Literal(lang)))\n",
    "        except LangDetectException:\n",
    "            print(\"Unable to detect language\")\n",
    "        count+=1\n",
    "    if(flag==3):\n",
    "        validDate = checkDate(df.iloc[:,i])\n",
    "        if(validDate != True):\n",
    "            g.add((column_uri, CSVTORDF.invalidDate, Literal(True, datatype=XSD.boolean)))\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rownum, row in enumerate(rows, start=1):\n",
    "    row_uri = CSVW.Row+f\"={rownum}\"\n",
    "    g.add((row_uri, RDF.type, CSVW.Row))\n",
    "    g.add((table_uri, CSVW.row, row_uri))\n",
    "    g.add((row_uri, CSVW.rownum, Literal(rownum, datatype=XSD.int)))\n",
    "    for colnum, value in enumerate(row, start=1):\n",
    "        cell_uri = CSVW.Cell+f\"={rownum}-\"+columnNames[colnum-1]\n",
    "        column_uri = CSVW.Column+\"_\"+columnNames[colnum-1]\n",
    "        g.add((cell_uri, RDF.type, CSVW.Cell))\n",
    "        g.add((row_uri, CSVTORDF.hasCell, cell_uri))\n",
    "        g.add((column_uri, RDF.type, CSVW.Column))\n",
    "       # g.add((row_uri, CSVW.describes, column_uri))\n",
    "        g.add((cell_uri, CSVTORDF.belongsToColumn, column_uri))\n",
    "        if(value == \"\"):\n",
    "            count=count+1\n",
    "        elif(intType.match(str(value))):   \n",
    "            g.add((cell_uri, CSVTORDF.hasValue, Literal(value, datatype=XSD.integer)))\n",
    "            count=count+1\n",
    "        elif((dateType1.match(str(value))) or (dateType2 .match(str(value)))):\n",
    "            g.add((cell_uri, CSVTORDF.hasValue, Literal(value, datatype=XSD.datetime)))\n",
    "            count=count+1\n",
    "        elif((uriType.match(str(value)))):\n",
    "            g.add((cell_uri, CSVTORDF.hasValue, Literal(value, datatype=XSD.anyURI)))\n",
    "            count=count+1\n",
    "        elif((stringType.match(str(value)))):\n",
    "            g.add((cell_uri, CSVTORDF.hasValue, Literal(value, datatype=XSD.string)))\n",
    "            count=count+1\n",
    "        elif((floatType.match(str(value)))):\n",
    "            g.add((cell_uri, CSVTORDF.hasValue, Literal(value, datatype=XSD.float)))\n",
    "            count=count+1\n",
    "        else:\n",
    "            g.add((cell_uri, CSVTORDF.hasValue, Literal(value, datatype=XSD.string)))\n",
    "            count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.w3.org/ns/csvw#Tableiris\n"
     ]
    }
   ],
   "source": [
    "print(table_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.serialize(\"./TTL files/iris.ttl\", format=\"ttl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "* Owlready2 * Running Pellet...\n",
      "    java -Xmx2000M -cp /home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/xercesImpl-2.10.0.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/jgrapht-jdk1.5.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/jcl-over-slf4j-1.6.4.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/xml-apis-1.4.01.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/httpclient-4.2.3.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/commons-codec-1.6.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/antlr-3.2.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/jena-core-2.10.0.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/jena-arq-2.10.0.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/httpcore-4.2.2.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/slf4j-api-1.6.4.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/log4j-1.2.16.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/jena-tdb-0.10.0.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/antlr-runtime-3.2.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/jena-iri-0.9.5.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/owlapi-distribution-3.4.3-bin.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/pellet-2.3.1.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/aterm-java-1.6.jar:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/pellet/slf4j-log4j12-1.6.4.jar pellet.Pellet realize --loader Jena --input-format N-Triples --infer-data-prop-values --ignore-imports /tmp/tmplhde_ly5\n",
      "* Owlready2 * Pellet took 0.7029023170471191 seconds\n",
      "* Owlready * (NB: only changes on entities loaded in Python are shown, other changes are done but not listed)\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import *\n",
    "filetoread = \"file://\"+os. getcwd()+\"/TTL files/randomData2.ttl\"\n",
    "onto = get_ontology(filetoread).load()\n",
    "\n",
    "#with onto:\n",
    "#    Imp().set_as_rule(\"Table(?table), Row(?row1), row(?table, ?row1), Column(?col), Cell(?cell), describes(?row1, ?col), contains(?col, ?cell), datatype(?cell, ?a), \")\n",
    "\n",
    "sync_reasoner_pellet([onto], infer_data_property_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Running HermiT...\n",
      "    java -Xmx2000M -cp /home/d19125691/.local/lib/python3.6/site-packages/owlready2/hermit:/home/d19125691/.local/lib/python3.6/site-packages/owlready2/hermit/HermiT.jar org.semanticweb.HermiT.cli.CommandLine -c -O -D -I file:////tmp/tmpje431t5a\n",
      "* Owlready2 * HermiT took 0.3672597408294678 seconds\n",
      "* Owlready * (NB: only changes on entities loaded in Python are shown, other changes are done but not listed)\n"
     ]
    }
   ],
   "source": [
    "sync_reasoner([onto])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".deeplearningClass",
   "language": "python",
   "name": ".deeplearningclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
